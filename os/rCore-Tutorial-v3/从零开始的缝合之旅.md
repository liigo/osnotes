# 从零开始的缝合之旅

## Log

稍微整理了一下思路，解决了一些 K210 的陈年老坑（并没完全解决），现在决定将第二版、第三版、ucore、xv6 完全缝合起来。

但是理智告诉我在几天之内是肯定缝合不完的...

我还是继续在 multicore 分支上缝缝补补吧 QAQ。虽然那个代码我自己都有点看不下去了。但是仔细想了一下确实是从头再来更好...不然有点改不下去。

### 2020-10-18

如果不快点干应该干不完了...QAQAQQQ这几天要进入刷夜模式了。

规定一下不动的东西：

整个文件系统模块原封不动，但是可能需要加一些新的 syscall。

### 2020-10-19

凌晨。惊讶的发现从头开始又是走了一条老路，如果让我真正从头再来的话我肯定能写出来...但是现在没有时间，照着之前的代码复制也是陷入了陷阱。所以按照计划进度没有跟上，我们在之前的分支上实现 syscall！

### 2020-10-23

这几天在 multicore 分支上基本上实现了一些 syscall，然后大多数 ucore 也都能够跑起来了。然后这个时候我们可以重新考虑一下如何重写 tutorial 并补充一些文档了。经过这几天对 syscall 的实现，目前大概能够明确以下几点：

1. 我们**必须**实现内核和用户地址空间的隔离，二者只会共享一个 trampoline page，里面是 trapframe 保存/切换的代码，它们需要被映射到内核/用户地址空间中的同一位置。在设置 stvec 的时候，需要设置为内核/用户地址空间中入口的相应虚拟地址。

   需要说明的是，在 K210 板子上，即使这一步稍显复杂也是必须要做的。不然，在不借助虚存的情况下能够同时存在的进程数堪忧，完全不能正常跑 ucore 的测试。这主要是因为需要映射的外设区间范围太大。如果只在内核地址空间中映射一下则是可以接受的。

   用户地址空间中的内容：最高的地方是一个 trampoline page，然后从某个地方开始向下是一个用户栈。逻辑上，在它的下面需要加上一个 guard page 进行保护。

   内核地址空间中的内容：还是和之前一样把物理内存线性映射到 0xFFFFFFFF80000000 里面。然后 0x0-0x80000000 这段是外设 MMIO 的恒等映射。至于为啥是恒等映射，完全是 k210 的现有能用的驱动程序都是在 M 特权级运行的嵌入式风格的。和用户地址空间一样，最高的地方也是一个 trampoline page，然后各种进程的内核栈，每个下面都带有一个 guard page 进行保护。

   > 这个其实要内核来保证在内核空间进行的 trap 处理都不会爆栈。可能已经有某种方法能够检测到这一点。如果已经确认不会爆栈，那么这个 guard page 也没有意义了。目前我们只是将其作为一种十分简单的检测手段。

   为此，我们需要一个一致的进程资源管理器。它负责分配/回收 PID，以及相对应的位于内核地址空间内的进程内核栈，还有用户地址空间和其他资源。

   那么问题就来了，如果有内核线程的话该怎么办？啊@_@，到底怎么从 C 和 Rust 里面取舍啊...QAQ

## lab1：最小化内核&格式化输入

这回尝试默认变成 release 模式，有一个问题就是会多出一个 .eh_frame 段，要把它手动丢到 .data 里面去，否则就会直接取代入口点。

想了一下，暂时不实现 logger 了。最后再说。

## lab2：Trap

直接访问物理地址+只涉及内核态+实现 ebreak/时钟中断

trapframe 里面保存 sstatus/sepc 首先可以为用户/内核线程构造一个入口。还有就是能支持某种程度上的 trap 嵌套。最有可能出现的一种 trap 嵌套的情形是：用户通过 ecall trap 进来，然后由于这个 ecall 运行的时间可能很长，我们需要打开中断。这里保存 sstatus/sepc 只是能保证在 sret 之后能回到正确的地方。也可以说在 trap handler 可能覆盖掉 sstatus/sepc 故而需要保存。有点把自己绕进去了...

这样说起来，为何 scause 和 stval 无需保存？应该是因为在 ecall 的时候，我们通过 scause 完成分发，参数的话只需要读寄存器即可，我们之后都不会再用到 scause 和 stval；如果是其他 trap，我们根本不会尝试重新打开中断，也就不会产生 trap 嵌套，更无需保存了。

RISC-V 应该避免出现**中断**嵌套。这是目前我所了解到的。

这次我们直接放弃保存 x3 和 x4，为后续的多核做铺垫。（等等，那如果什么地方修改 x3 和 x4 要怎么办，忽然想到好像没有相关机制保证 x3 和 x4 不能使用。不过这个 thread pointer/global pointer 编译器应该不会乱用，姑且相信它吧）

另一个问题是，我们需要保存 sp 吗？当时这段汇编写的很复杂，希望能尝试简化一下...令人最不爽的点在于，sscratch 和 trapframe 里面的 sp 到底使用哪个。我们可以将奇怪的通过 trapframe 第一次进入用户态与其他独立起来起来，因为只有它非常奇怪（令人很不爽的是，它还可以通过 trapframe 进入内核态）。

剩下的可能比较简单，只需将 s 进入/退出 s，和 u 进入/退出 s 合并到一起。

现在没有时间仔细推敲 trap 的汇编代码了。只是照抄了第二版的代码之后把 breakpoint 和时钟中断跑起来了。略微调整了 k210 的频率常数，现在能做到接近一秒输出一次 100ticks。所以它的真正时钟频率只有 10M 吗...现在每个 tick 是 10000 个时钟周期，应该能看到比较明显的多核切换。

## lab3: 内存管理

这里实现物理页帧管理和内核堆内存分配即可。

# 续命的缝合之旅

## Log

### 2020-10-18

仔细想了一下，距离 22 号的 ddl 已经非常接近了QAQ，从零开始有点开玩笑了。那真就是只能改动现在的 multicore 分支，依次完成下面这些任务：

1. 将中断和调度之间的耦合删除，每个用户进程一个内核栈，对于中断、进程、调度三个方面都要进行改写QAQ。要保证修改之后还能够正常跑目前的三个应用程序。
2. 完善虚拟存储支持来跑比较大的用户程序。
3. 完整支持 fork/wait/exec 系统调用，能给函数传入命令行参数
4. 无需拔插 microsd 即可烧写文件系统的小工具
5. 完善文件系统支持，参考 xv6 提供 pipe 支持，从而终端可以通过 `|` 实现简单的 IPC。

那么就分成 4 个不同的 step 开始干活吧！

然后发现从中断开始就根本干不下去，应该不可能同时把中断机制换掉了。先按照原定的计划从头开始来，如果明天上午发现进度跟不上的话就切换到在原来的 multicore 分支上完善 syscall。

在进度的压力下，至少在 22 日之前我们不考虑 step 1 了！而且需要重新调整一下完成顺序：

1. 完整支持 fork/wait/exec 等 syscall，能够跑 xv6 的一些用户程序。
2. 完善文件系统，支持 pipe，从而能够跑 xv6 的更多用户程序。

### 2020-10-19

现在是晚上十点半。已经可以在内核里面通过硬编码跑一个 `forktest`，验证了 `fork` 和 `wait` 实现的正确性。

接下来就是要正确实现 `exec`，然后替换掉 user_shell 的实现。

### 2020-10-20

现在凌晨一点，user_shell 已经替换成基于 fork/exec 的实现。但是现在有一个问题就是 threadtrace 会出现某一个 hart 上面用时特别长的情况。看了一下是 exec syscall 的时间都计入了内核态，现在的实现是需要直接将所有页面拷贝到内存的，所以直接就几百万个时钟周期了...这个后续的实现我们还可以再改进一下。

### step1(已弃坑)

首先是根据我希望的命名，将 `interrupt` 目录改成 `trap`，同时将 `context.rs` 重命名为 `trapframe.rs`，将 `interrupt.asm` 重命名为 `trap.asm`。

接着，将 trap 分发入口 `handle_interrupt` 重命名为 `trap_handler`，将进入和退出 `trap` 的汇编代码重命名为 `__trapentry` 和 `__trapret`。

重要的是，`handler_interrupt` 不再需要返回一个 `*mut Context`。像是第二版一样，它仍然需要一个 `*mut Context` ，也就是位于内核栈顶的 TrapFrame 作为输入参数。在 `trap_handler` 里面，大概的逻辑是

### step1

首先是调整用户态的代码/数据段的位置。在 user 文件夹里面新增 linker.ld 将开头位置调整到 0。这样和 xv6 保持一致。此外根据先前的设计我们希望用户栈从 0x0c00_0000 向下增长，这样有 192M 的虚拟地址空间。目前来讲完全够用了。

发现之前的设计里面映射的 DEVICE_MMIO 区间实际上是给 alloc_page_range 使用的，跟串口不沾边。因为串口纯粹通过 sbi 接口，在 S 态根本不用映射。

现在我们调整了运行栈的位置，并替换了 alloc_page_range 函数为实际中会用到的 alloc_run_stack 函数。在 ProcessInner 维护一个 run_stack_pointer，每次从这个开始往下分配一个 run_stack 在加上一个 protect page。内核进程从最高的第二页往下，而用户进程从 0x0c00_0000 开始向下。

加上一个 Segmentation fault 机制：目前是只要在用户态触发 page fault 就直接杀死当前进程。目前加入了一个 stack_overflow 程序，在两个平台上可以正常跑了。

接下来，我们需要正确实现 fork/wait/exec/exit 的语义，尤其需要注意父子进程机制。

先定义一下父子进程机制：父进程通过 fork 可以创建子进程，在父进程里面维护子进程的一个 Arc。父进程可以通过 wait 得到一个已经退出的子进程的 PID 和返回值。在得到它之后，我们会将这个 Arc 删除掉，于是所有进程的所有引用计数全部消失，可以回收掉相应的资源（事实上，在进程退出的那一刻开始我们就应该回收掉）。

fork：将子进程的 Arc 放入父进程的子进程列表。

exit：将返回状态保存在当前进程的进程控制块中，回收掉用户态的所有物理页面（即用户栈和用户代码、数据段，即 0x0c00_0000 以下的部分），保留页表里面其他的映射。查看父进程的状态，如果它正在 waiting，则唤醒父进程。*这里需要特别小心锁的设计，包括：整个调度队列、ProcessInner，还有 wait 相关的 sleeplock 应该如何设计。*

wait: 

**Segment::map** 可能有 bug。可能是 xv6 应用太毒瘤了？怪事，xv6 内核里面判定了每个段的开头必须页对齐，但是反汇编出来就不是这么回事了...但是我们暂时不用管它。

fork 的实现方法：初始化 memory_set，然后把 0x0c00_0000 以下的所有段都复制一遍：由于我们在父进程的页表里面，这很好办，...现在地址空间就算是齐活了。然后需要构造一个合适的中断帧，就和父进程的中断帧一样就行了，但需要将返回值 a0 改成子进程 ID。这样好像就行了啊。现在加上了 fork 和 getpid，比想象中容易实现很多啊。下午先来写一个 log 区分用户和内核的输出。

Logging 模块终于加上了。现在要面对的就是父子进程的处理...

目前已经实现的机制：

异常或者调用 exit 之后在 PCB 里面更新 exited 和 xstate 状态，立即回收掉用于存储用户数据的所有物理页帧。

终端进程作为根进程，进程最终都会被移交给它来做回收。

在 fork 的时候更新父进程的 child 和自身的 parent。需要注意的是，作为 root 的 user_shell 里面的 parent 为 None，而在 fork 里面调用 Process::from_parent 的时候我们已经在子进程里面保存了父进程的 Weak。

正确实现 wait 调用：只需在 PCB 里面查找一个 exited=true 的子进程，取出 xstate，将对应的 `Arc<Process>` 丢掉，如果找了一遍都没有找到的话需要被阻塞。如果调用的时候发现子进程列表为空，则直接返回 -1。目前是返回 -2 的话需要回到循环开头 retry。之后将 Syscall::Park 的参数去掉之后可以稍微解决这个问题。

当一个进程退出时，如果发现父进程正处于等待子进程退出，也即 wait 状态，则唤醒父进程；这个我们可以通过在 ProcessInner 里面加入一个 condvar 来实现。

理论上来讲，Syscall::Park 无需带参数。反正它都是要回去重新执行。目前的 fork/exec 还有点问题不太支持。

尚未实现的机制：

在fork 的同时复制 Unix 资源。

当一个进程退出时，如果子进程列表不为空，则将子进程移交给一个专门的回收用户进程。但是目前，我们暂且假定每个进程要负责等待它 fork 出来的所有的子进程结束。



到这里为止，step1 要做的事情基本上做完了。

### step2

简单列一下 step2 的目标。

1. 支持 exec 的命令行参数；
2. 仔细看了一下 ucore，发现里面还用到了 gettimeofday, yield, 还有 sleep。
3. 提供文件系统的 syscall，尤其是需要支持 pipe。为此，在 fork 的时候需要正确复制 Inode。
4. 在此基础上，支持虚拟存储和物理页面的延迟分配。

### ucore 应用移植

我们需要将原来 ucore 的测试跑起来。那么首先就是需要将 ucore 的测试程序拿进来。

从 rcore-user 里面搬运了一下 rust 和 ucore 两个目录的构建逻辑，然后有一些奇怪的 bug...

将 find 替换成 lookup，因为我们不再只是在根目录下面了，而是需要形如 `rust/hello_world` 这样的输入，发现会在奇怪的地方出现死锁。

ucore 应用会出现 filesz < memsz 的情况，elf 库不会帮我们 padding 一些 0，只能靠我们自己来做。

在移植的时候似乎做了这些事情：

1. 处理 filesz < memsz 的情况，原先的第三版不能处理 filesz < memsz 但 filesz > 0 的情况
2. 在 trap 上下文中保存 gp，因为 ucore 用户程序用到了它
3. 将 ucore initcode.S 中对于 a0,a1 的初始化功能移除，稍后移动到内核
4. 更新 sys_wait 的语义，现有两个参数 pid 和 xstate，如果 pid = 0 的话则代表可以等待任意一个子进程结束，否则只能等待一个 pid 给定的子进程结束才可以返回；而 xstate 也有可能为 NULL，这个时候 kernel 不应该向里面写入子进程返回值，否则会 page fault。

目前可以成功运行的 ucore 程序：

* hello
* forktest
* divzero
* testbss
* faultread
* faultreadkernel
> 说明：以上两个测例没啥营养...而且 faultreadkernel 的地址目前并不在 kernel 之内。
> 

* exit：只需要实现 yield 即可（从这里开始还没有在 k210 上测试过）
* matrix：似乎只需要实现 yield 即可
* yield：需要实现 yield。
* badarg: 需要实现 yield，且需要判断传入的 `*xstate` 用户态可以访问；
* sleep: 需要实现 sleep 和 gettime_msec。
* forktree：需要实现 yield 和 sleep。sleep 的时间单位为一个 tick，也就是 10ms。
* spin: 需要实现 kill 和 yield。


暂时不能运行的测试程序以及相应的 feature：

* sleepkill：需要实现 sleep 和 kill。而且，在一个进程处于阻塞状态下被 kill 的时候，我们大概可以选择：将其标记为 kill，找到它所在的 condvar，并唤醒它。
* waitkill：需要实现 yield 和 kill，其中一个子进程会杀死父进程，所以需要更加完善的进程移交机制。我们需要初始化一个专门 `while (wait())` 的初始用户进程。然后如果一个进程在退出的时候发现它有一些子进程，直接将这些子进程移交给这个初始用户进程。

* priority：需要实现一个 ucore lab6 特有的设置进程优先级的操作...此外还需要 sleep。这个需要修改进程调度框架，目前应该先不考虑。
* sfs_filetest1：只读的方式打开一个硬编码的文件然后 stat，没啥用...
* ls：一大堆文件系统相关的 syscall
* sh：一个简单的 shell，支持重定向操作 `<,>` ，管道 `|`（现在的实现没用管道而是类似 dup）。进程调度相关的东西目前都应该已经搞定了。整体上这个要放到最后。


x86 独有，应该从 rv64 的测例中移除：
* badsegment
* softint

暂时在 k210 上还跑不起来的测例：

* testbss：数组开得太大了...
* forktest：fork 的数目太多了

对于测例进行的改动：

* forktree：目前 tutorial 要求每个进程负责等待所有他 fork 出来的子进程结束才能退出。因此，在 `forktree()` 结尾加入了一个 `while (wait() >= 0)` 来等待所有子进程结束并回收资源。此外，将 DEPTH 调整为 3 以适应 K210 的 8M 内存。
* 将 forktest 中的子进程数调整为 15，以适应 K210 的 8M 内存。
* 将 matrix 中的子进程数调整为 15，以适应 K210 的 8M 内存。
* testbss 之前开了 4M 的数组，太疯狂了，现在改成 1M。

> 注：目前 K210 上对于设备的映射会造成存储页表页面的极大浪费，现在一个进程会在内存中保留 60 个左右的页面，而整块内存的线性映射只会占用 7 个页帧。这个之后完成用户、内核地址空间的隔离会缓解这一问题。目前只能先将同时存在的子进程数调整的小一些。
>
> 目前暂定的 MMIO 区域是从 0x0c00_0000 直到 0x80000000。一个一级页表项可以解决 0x40000000 到 0x80000000。那么还剩下 0x0c00_0000 直到 0x40000000。一个二级页表项只能解决 2M，而这些加起来一共 3*256M+64M=768+64M=832M，因此需要 416 个二级页表项，事实上我们知道它是一个二级页表的后缀。如果这样的话外设部分基本做到零开销了。
>
> 

### syscall 备注

* 现在的 sys_kill 可能还没办法 kill 掉一个正在阻塞的进程。现在的实现方法就是在进入 trap 的时候检查一下当前 process 的 killed 有没有被标记为 true。如果是的话则等同于触发了 page fault。

  事实上我们还是需要像传统的实现那样有一个 process table，我们需要根据 pid 查到进程的状态，如果是阻塞的话，需要保存指向 condvar 的指针。这样 kill 的话就可以把阻塞的状态取消，然后再回收掉进程。

  目前为了简单起见，我们搞一个 process table，但并不处理进程正在阻塞的情况。
  
* 准备开始创建 initproc 作为初始用户进程，它 fork 并 exec user_shell，随后一直 wait 来回收掉所有僵尸进程。但是如果想实现这个机制的话就需要将 parent 移动到 Process.Inner 中...假如父进程退出，需要修改子进程的 parent，而如果子进程这个时候正在 exit，需要尝试唤醒对应的父进程，这明显会产生并发冲突。但是这样做了之后发现内核进程报 page fault 了...还是在我删除了子进程转移相关逻辑的情况下。想正确实现并发真的很困难，直接将所有可变的东西丢到一个 Inner 并不够，事实上我们必须知道 Process 的某种“不变量”，在不变量可能不满足的中间状态必须全程持有 Inner 的锁。而之前的实现很有可能是修改了 Inner 中的某个地方就将锁释放掉，这个时候是破坏掉“不变量”的。一旦在多核的情况下，就很容易导致整个系统的崩溃。

  这里也许可以看出 Rust 相对于 C 的一个小缺点。在 C 里面我们显式获取并释放锁，而在 Rust 里面它是相对隐式的，这要求编程者在处理的时候要有更加清醒的头脑才行。

  上面的内核进程 page fault 是一个非常诡异的情况：

  ```
  Process 2 Segmentation Fault cause = Exception(InstructionPageFault), vaddr = 0x8, sepc = 0x8 @Core0
  ```

  甚至在一开始 __restore 到 idle 线程的时候就可能会报错。

  甚至，在 ProcessInner 里面加上一个初始化之后就不再使用的 parent_pid 都会爆炸，原先 usertests 可以跑，现在则可能出现死锁。这完全有点玄学...
  
  补充：上面这个诡异的卡死 bug 是内核栈开的不够大导致的。在今后的设计里，我们一定要给所有栈都加上 guard page，包括每个线程的用户栈和内核栈。

### 奇妙的卡死 bug

在 K210 上运行 matrix 等程序的时候常常能够观察到板子直接卡死，而并不是在内核里面触发了死锁。其具体表现为某个进程正常返回用户态之后就不再能够观察到它通过系统调用或者时钟中断回到内核态。

1. 可能是返回的时候 trapframe 的内容有误？但是我检查过返回的时候和之前被换出并保存的时候的内容是一样的。不太可能出现保存的时候就是错误的情况。
2. 假设能够正常回到用户态。由于用户态的程序没有问题，最终一定能够通过 syscall 或者时钟中断回到内核。所以我比较倾向于它并没有正常回到用户态。

这个问题比较诡异...

目前有一个发现，就是第一次运行 matrix 程序的时候都没有问题（连续运行 10 次），而第二次运行的时候就可能会出现问题。这可能是资源回收的锅。首先尝试显式在分配一个物理页帧之后将其清零。但是问题并没有解决。

这个第一次运行都没有问题可能是一个很重要的发现吧...但是目前实在调不下去了...现在准备重写代码、文档的同时看看能不能运气爆表顺便修复掉这个 Bug 吧。究竟是不是运气问题呢？

甚至还观察到在 debug! 的期间停住。

更新：目前已经发现了一例（？）首次运行 matrix 就直接卡死的情形了。

现在在 k210 上只启用单核，然后 matrix 里面开 15 个子进程，由于程序随机种子相同，内核的调度算法也相同，以及一些其他的相关参数也相同，每次成功的运行都是如下的情况：

```
>> matrix
pid 4 is running (1100 times)!.
pid 4 done!.
pid 5 is running (17900 times)!.
pid 6 is running (3500 times)!.
pid 7 is running (5900 times)!.
pid 8 is running (2600 times)!.
pid 9 is running (13100 times)!.
pid 10 is running (15400 times)!.
pid 6 done!.
pid 8 done!.
pid 11 is running (7400 times)!.
pid 12 is running (20600 times)!.
pid 13 is running (1000 times)!.
pid 13 done!.
pid 14 is running (13100 times)!.
pid 7 done!.
pid 15 is running (1100 times)!.
pid 15 done!.
pid 16 is running (1100 times)!.
pid 17 is running (13100 times)!.
fork ok.
pid 16 done!.
pid 18 is running (1100 times)!.
pid 18 done!.
pid 11 done!.
pid 9 done!.
pid 5 done!.
pid 14 done!.
pid 10 done!.
pid 17 done!.
pid 12 done!.
matrix pass.
Shell: Process 3 exited with code 0
```

如果卡死的话，则在屏幕上会显示完整状态的一个前缀。

目前的卡死应该和内核栈的大小（已经开的很大）没有关系。

甚至还出现过这种极端情形...

```
>> matrix
pid 12 is running (1100 times)!.
```

没错，它卡死了！

尝试将 read_time 改成直接访问 clint 而不要经过 M 态。但是并没有啥用。怀疑还是时钟中断的任务切换出了问题...？但是到底能出什么问题呢？

我特意去看了一样 matrix 的反汇编，确认里面并没有使用 tp 寄存器。而 r_user_shell 里面也并未访问。

然后仔细看了一下，__restore 应该没有什么会导致死循环的操作...

然后尝试一下不要回收 pid 和 PROCESS_TABLE 试一下。跟这个没有关系。

调试了一天还是未果QAQ。主要这个东西也很难硬件调试啊...软件的话实在想不到怎么调试了...

在 10 月 26 日，貌似终于找到了一个可以稳定重复的卡死情况！而且用了一些其他的测例，感觉并不是硬件问题，还是软件问题QAQ。好像又不能稳定重复了...这到底是怎么调试啊...

而且发现另一个可能比较好调试的问题：一开始输入命令的时候就已经有可能卡死了。这个应该是不涉及用户态的。这个好像没有调试输出的时候就没有了...

目前将 matrix 改成了 Rust 版本似乎不会卡死了。但是另一个 Rust 版本的 forktest2 在 k210 上可能卡死，且在 qemu/k210 上都会出现行冲突的情况，比如：

```
Subprocess Subprocess 1514 OK!
 OK!
```

而我们实际上是在输出一行字符串的过程中全程持有锁，即使是多核也不应该出现这种情况。这个问题还需要解决一下。

而且某些（比如 matrix）的测试单独运行可以，放在 usertests 里面就也会卡死。

现在 10 月 27 日，整理一下目前的状况：

在 qemu 平台上，r_usertests_full 运行到 matrix 的时候观察到 ProcessInner 死锁，也许可以解决部分问题。发现加上 log 的话就没有死锁了...

今天早些时候怀疑是 k210/qemu 对于原子指令（amo/lr/sc 系列指令）和内存屏障指令（fence）的支持不相同，导致会出现输出行冲突的情况。但这个思路很难继续下去。

用 Rust 重写了一些 ucore 测例，如 sleep/yield/exit/spin 等等。目前在 k210 上稳定可以发现第一次 r_usertests 可以正常运行，第二次运行的时候则必定在 r_forktest 处卡死。尝试一下 qemu。在 qemu 上未出现此现象。尝试将 qemu 的配置与 k210 完全拉平再尝试一下。

在调整 qemu 配置的过程中，发现 qemu 版的起始物理地址是 0x80200000，大概多出了接近 2M，因此最终的物理地址还是设置为 0x80a00000 而不是和 k210 一样的 0x80800000。同时，将 qemu 的配置也设置为双核，内核栈的数量调整为 2。每个 hart 的启动栈大小和 k210 一样调整为 64K。结果是跑 r_usertests 速度突然变得飞快，且连续运行 5 次 r_usertests 均未出现问题。但是仍然有可能出现问题：在 r_forktest 中间或者在 r_sleep 没有任何输出的地方卡死。

目前的重点排查范围：原子指令的有效性（通过多进程互斥计数），时钟中断。

Debug 了一下发现锁并没有问题（通过 r_atomic 测试程序测试多进程互斥计数）。然后，会出现行冲突的原因在于加锁的粒度不够，事实上在 println! 的时候会将字面量和变量分段进行打印，比如这样：

```
[Shell: Process ][3][ exited with code ][0]
```

我们只是给每一段的打印加了锁，因此确实会出现行冲突。

在 k210 上运行规模较大的 r_atomic 再次大概率出现卡死现象，这里开 13 个进程，每个进程调用 10000 次 count 系统调用。但如果是进程数较少，在 r_atomic2 里面一共只有两个进程，每个进程调用 100000 次 count，可以稳定通过。这说明卡死现象几乎只会发生在并发进程极多的情况下？从 r_atomic/r_forktest2 大抵能够发现这一点，但是也曾经出现过在运行打包测试的后面一部分，实际上是比较简单的测例的时候卡死。