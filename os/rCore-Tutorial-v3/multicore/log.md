## 多核相关调研

多核的内容究竟如何体现，出现的顺序如何，以及与进程/线程等如何结合是一个问题。虽然需要把代码写完之后再考虑如何写文档，但是可以先调研一下别人是怎样做这件事的。

### MIT xv6 for RISC-V

[6.828 Fall2018 lec9: Multiprocessors & Locking](https://pdos.csail.mit.edu/6.828/2018/lec/l-lockv2.txt)

**小作业**

* 简单的多线程 Hash 表，不加锁的情况下同时往同一行插入就会触发 race condition

  race condition 其实就是至少有一个操作为写入的并发访问

* 粗粒度与细粒度的锁的优劣比较

* 子任务数量至少要多于核数才能得到更好的加速比

* 某些情况下，即使出现 race condition ，也不见得结果就是错误的。但是这可能需要比较复杂的讨论或者是形式化验证

**锁**

* 临界区，只有一个线程可以进入

* 不同的锁保护不同的数据/临界区

* 何时应使用锁：一种最典型的情况是——同时有多个线程访问共享数据，且至少一个是写入；但是这个标准有时过于严格，有时又有些宽松

* 如果编译器自动帮忙添加 acquire/release pair，有些情况下会显得不够灵活（不过 Rust 中的 Mutex 实际上就是这么干的...），比如整体需要封装成一个原子操作，可能做不到

* 锁的作用：构建原子块，维护数据结构中的不变量(?)

* 问题：死锁

  其中一种解决方案：构造一个所有数据结构的偏序集，按照偏序申请锁资源

  事实上，需要在代码模块化与锁机制中间进行权衡（这是因为，我们需要知道每个函数 acquire 了哪些锁来保证不会出现死锁，这并不是在模块内部就能解决的）

**锁与并行**

* 锁的目的实际上是避免并行访问

* 因此在设计上要通过数据和锁的合理划分来做到同一时刻各线程访问的数据互斥

  想要做到尽可能细的粒度比较困难

  文件系统：整个文件系统 -> 文件/目录 -> 块

  内核：整个内核 -> 子系统 -> 内核对象

* 有一种手段是设置 core-local 的数据结构？

* 糟糕的地方在于，在多核背景下，需要考虑用锁来保护的情况有很多类

**锁的实现**

* 基本还是基于一些原子指令
* 如果使用锁的话，就无需考虑 memory ordering(可以理解成加入各种不同的屏障尽可能降低指令乱序执行的影响) 的问题；但是如果不使用锁而基于 lock-free 的数据结构的话，那么必须了解 memory ordering 的所有细节
* 目前还是只讲到 spinlock

**小建议**

* 减少不必要的数据共享
* 从粗粒度的锁开始设计逐渐细化
* 通过实际测试来验证性能而不是目测
* 使用自动化工具如冲突检测

**课后练习**

* 中断与锁的联合...，这个我们暂且抛下不谈

[6.828 Fall2018 lec10: Processes and switching](https://pdos.csail.mit.edu/6.828/2018/lec/l-threads.txt)

* 进程认为自己独占 CPU 和内存，这是通过虚拟化/隔离来实现的

* 进程相关 API: fork/exec/exit/wait/kill/sbrk/getpid

  吐槽一下，在新版中是否还要提供这些系统调用需要讨论下

* 挑战：进程的数量多于 CPU 的数量，需要将 $N$ 个 CPU 复用到 $M$ 个进程上面，因此需要时间片共享、调度、上下文切换

* 目标：用户进程透明、用户进程与内核均可抢占

* xv6 的解决方案：每个进程一个用户线程和内核线程；$N$ 个 Core，每个 Core 一个调度器

* 线程：一个 Core 的执行流（包括寄存器和栈），可能正在执行，也可能被保存下来留待以后执行

  这里是说所谓的执行流，还是说表示成图灵机上的一个状态呢...?

* xv6 上下文切换流程（以一次系统调用为例）

  * 用户线程通过中断或系统调用切换到内核线程
  * 内核线程被抢占或等待 I/O 交出 CPU 使用权
    * 内核线程切换到调度线程
    * 调度线程选出另一个就绪状态的内核线程
    * 调度线程切换到被选中的内核线程
  * 系统调用/中断处理完毕，内核线程切换回用户线程

* xv6 进程状态：运行、就绪、阻塞、僵尸、未使用

* 可惜的是，xv6 虽然支持内核“进程”下有多个内核线程，但是每个用户进程只有一个用户线程，也就是说不支持 pthread 那套东西了

  虽然如此，我们还是可以考虑用户态线程 greenthread

* 上下文切换为何在 xv6 中如此困难，是因为同时要考虑到：多核、锁、中断、终止进程(?)

* 有点看不下去了，暂时先停下来

[6.828 Fall2018 lec11: sleep & wakeup](https://pdos.csail.mit.edu/6.828/2018/lec/l-coordination.txt)

[6.828 Fall2018 lec18: scalable locks](https://pdos.csail.mit.edu/6.828/2018/lec/l-scalable-lock.md)

[xv6 2018 实验指导书](https://pdos.csail.mit.edu/6.828/2018/xv6/book-rev10.pdf)

这里将一些值得注意的信息或命题记录下来。

**Locking**

* 即使单内核，也会由于中断的存在导致数据访问冲突。

* 并行 (parallelism) 与并发 (concurrency) 的不同在于，并行强调多个线程**同时**运行，并发则相比并行更加一般化，只要有多个线程在干活就行，但不见得是同时干活，可以通过上下文切换达成 CPU 资源虚拟化

  事实上，二者大概不算是同一层上的概念？

* 如何在这种并发访问的情况下保证正确性（或称一致性？）以及足够的性能？

* 使用锁来提供互斥访问

* 需要注意观察：另一个 Core（或者中断）是否能够通过修改锁保护的数据（或硬件资源）来使锁期望的行为发生变化

  一条 C 语句可能会被编译成若干条指令，我们必须注意这些指令运行到一半的中间状态，如果另一个 Core 介入或是通过中断跳转到另外的地方，是否会对原状态产生影响？也就是说，这些语句并非原子的。

* race condition：考虑多个 Cores 共享一个 IDE 磁盘，在驱动中有一个链表来记录所有 Cores 传过来的请求，Cores 会并发的加入请求

  如果处理不当的话就会导致写入丢失或者读到不完整的数据的错误

  错误是否发生取决于发生竞争的时间以及 CPU 如何乱序执行代码，因此很难复现与调试

* 锁保护的到底是什么？从某些观点来看，锁实际上保护的是数据结构中的某种所谓“一致性”或者不变性，即数据结构在进行一系列操作前后始终保持的特性。但是在操作中，这种不变性有可能会被破坏。此时，如果通过中断切换到的其他函数或者其他线程访问到了不变性被破坏的数据，就**有可能**会产生错误。

  可以说，这类操作需要作为临界区通过锁或者其他方式加以保护，从而将这些临界区的并发访问**串行化**，或者说将临界区变成**原子**操作。

* 锁的实现：简单的循环等待然后修改状态自己就会出现并发访问 bug，关闭中断（多核的话，中断也不是那么简单的事情了）只能用于单核，真正要支持多核的话还是需要硬件提供的原子指令的支持。原子指令的实现方式因架构而异，某一种实现方式是在总线上加锁，禁止其他的核对相关内存的修改。

  这里很可能还要参考一下计算机体系结构的相关内容。才能知道在硬件设计上是个怎样的想法。

  如此看来，并不能简单的给原子指令下定义。它与原子操作并不相同，并不是说不会被打断，而应该是说不会受到其他核心的影响。

* 单核 OS 拓展到多核 OS 的一种方法：进入内核时加锁，返回时释放

* 要按照一个全局偏序来请求多个锁避免死锁

* 如果中断服务例程中使用到了 spinlock，那么获取了相同的锁之后进入该中断将产生死锁。xv6 的解决方案是：在 spinlock 的临界区中全程关闭中断。

  可能会有多个临界区进行嵌套，方法是记录目前持有的 spinlock 的数量，一旦该数量变为 0 就重新打开中断。
  
* memory ordering：看起来这个也是跟多核的关系比较大。因为如果是单核的话，乱序执行并不会影响正确性。

* 由于 sleeplock 不关闭中断，它不能被用在中断服务例程中（可能是跟底层的 spinlock 有冲突？）；由于它交出了 CPU 控制权，他也不能被嵌套在 spinlock 的临界区中。但是反过来是可以的。

  事实上，在 xv6 中，sleeplock 仅用在耗时较长的文件系统操作上

* 锁的局限性：函数调用者和被调用者要对双方持有哪些锁做良好的约定

* 现实中，常常基于 lock 构建更加复杂的同步互斥原语，并需要合理使用并发冲突检测工具

  POSIX thread 需要 OS 的支持，**尤其是**当一个 pthread 修改了进程的虚拟内存空间时，需要通过 IPI 来通知其他 pthread 所在的核及时刷新 TLB（想想还真恐怖）

  如果多个核要**同时请求同一个锁**，锁的开销就会很大。这尤其体现在需要通过总线进行各个核之间 cache 的同步上。

  可以通过 lock-free 数据结构（例如：链表）来保证同步互斥，但是这比 lock 更加复杂，需要充分理解原子指令以及 memory ordering...

**Scheduling**



### OSTEP

[lec10 Multi-CPU scheduling](http://pages.cs.wisc.edu/~remzi/OSTEP/cpu-sched-multi.pdf)

### rCore_tutorial SMP by wrj

修改的部分如下：

* os/src/boot/entry64.asm

  将 hartid 存储到 a0 寄存器中，并为每个 hart 在内存中对应设置了启动栈

* os/src/memory/memory\_set/mod.rs

  在创建 MemorySet 时，将启动栈、PLIC、串口对应的 MMIO 加入

* os/src/memory/mod.rs

  将原有的 `init` 函数拆分为 bootstrap core 调用的 `init` 函数以及其他 cores 调用的 `init_other` 函数

  在 `init` 函数中仍进行内核重映射工作，并将新的 `satp` 保存在一个 全局变量 `SATP` 中；而在 `init_other` 函数中，要做的仅仅是将 `SATP` 的值写入其他核的 `satp` 寄存器中，让所有的核都看到相同的内核地址空间

* os/src/process/mod.rs

  将全局变量 `CPU` 替换为一个 `CPU` 数组，其中的每个元素对应一个 `Processor` 实例，其实也就是对应一个调度器，目前是每个核独立进行调度

* os/src/process/processor.rs

  首先，线程切换从 `Thread` 级的切换（`Thread::switch_to` 函数）去掉了繁琐的包装直接替换成上下文 `Context` 的切换（`Context::switch` 函数），因此 `ProcessorInner` 也不必保存 `idle` 线程的指针，而是直接保存一个调度函数 `run` 的上下文即可。

  还修改了 `yield_now, sleep` 的功能：目前 `yield_now` 的功能是在关闭中断的块中将控制移交给 `run` 函数，而 `sleep` 调用 `yield_now` 实现，先修改线程池中对应位置的状态信息，然后调用 `yield_now` 移交控制权。

  除此之外，总体功能跟之前是保持一致的。

* os/src/process/structs.rs

  移除了 `switch_to, get_boot_thread, append_initial_arguments` 等没用的函数。

* os/src/process/thread_pool.rs

  将 `Condvar::wait` 中的 `yield_now` 改成 `sleep`，因为二者的功能已经发生了变化。

* os/src/trap/trap.asm

  在里面针对 hartid 所在的寄存器 `gp` 进行了保存恢复，但是这操作没太看懂...

* os/src/context.rs

  删除了无用的 `Context::append_initial_arguments` 函数。

* os/src/init.rs

  目前，hart0 主要在 `rust_main` 函数中进行初始化，而 hart1~3 主要 `other_main` 函数中进行初始化，且 hart0 先完成内存、中断（包括 `init,init_board` 两部分）、文件模块、进程模块、时钟的模块初始化之后，其他 hart 才能开始初始化，分别使用 `interrupt::init,memory::init_other,timer::init` 进行初始化，随后（并不是严格的随后）所有的 hart 分别开始进入主调度函数

* os/src/interrupt.rs

  目前来看 `x3(gp)` 貌似存放的是 hartid，不知道是不是 OpenSBI 里面设置的。

  这里的 `init` 是将原来的实现拆成了两部分，修改寄存器的每个 hart 上面都要进行，修改 MMIO 的部分则只有 hart0 上进行。

* os/src/io.rs

  对于原先的 `Stdout` 包了一层 `Mutex`，防止多个线程同时通过它输出。

对于目前出现错误的分析：

用户终端的线程明明已经被其他核抢占了，但是接下来用户调用 `sys_read` ，在用户未输入任何字符的情况下，将对该线程进行 `sleep`，此时打印信息却显示该线程在 hart0 上运行，最终 panic 的直接原因是在尝试 `yield_now` 的时候， hart0 对应的 `Processor` 实例中的 `inner.current` 不存在，也即 hart0 上目前没有运行任何线程，并没有通过 `run` 从线程池中调度到 hart0 上。

1. 用户终端线程何时从 hart0 对应的 `Processor` 实例中移除：用户终端调用 `sys_read`，在内核中最终决定将该线程 `sleep`，也就是将线程池中该线程的状态修改为 *Sleeping*，然后 `yield_now`，在**关闭中断**的情况下切换到 `run`，这里通过 `take` 将 `Processor` 实例中的 `current` 还原为 `None`，并负责将该线程 `retrieve` 到线程池中。

   注意，`run` 里面是关闭外部中断的，因此可以注意到只能在 `enable_and_wfi` 中

2. 该线程的唤醒过程：当用户输入字符时，hart0 上触发外部中断，在 handler 中通过 SBI 获取到输入的字符，然后 push 到 `STDIN` 的队列里面去，同时调用 `wake_up` 唤醒终端线程，注意这里的 `wake_up` 是在当前 hart 对应的 `Processor` 实例上调用 `wake_up` 函数，不过最终都是所有 `Processor` 共享的底层 `ThreadPool` 上调用 `wakeup`，在其中将线程状态修改为 `Ready`，然后放进调度器中。

3. 终端线程如何恢复运行：终端在没有用户输入的时候应该被阻塞在 `yield_now` 的上下文切换中，切换回来之后依次从 `yield_now->sleep->Condvar::wait` 返回，回到 `Stdin::pop` 函数中，尝试从缓冲中获取字符，发现这次能获取到，于是成功返回，进而 `sys_read->syscall->interrupt::syscall->rust_trap` 依次返回，接下来通过 `RESTORE_ALL` 恢复到进入 Trap 之前的状态，并通过 `sret` 从 syscall 返回

注意到全部关闭时钟中断之后内核不会 panic，因此时钟中断对于整个流程产生了什么影响呢？

我们看到，在第 2 步之后，终端线程被抢占到其他 hart 上进行第 3 步，并能够看到它进一步通过 `sys_write` 系统调用输出了用户输入的字符，此时终端又发出了 `sys_read` 的系统调用，此时诡异的地方出现了，本来它应该是在其他 hart 中运行的，但从输出来看第 1 步仍然是在 hart0 上（或者说它认为自己在 hart0 上）的。但是实际上，它并不在 hart0 上，因为 hart0 对应的 `Processor` 实例中的 `inner.current` 为 `None`，应该也不太可能是通过时钟中断的 `tick` 又转移到 hart0 上，所以在尝试 `yield_now` 的时候 panic 了。

因此我们必须搞清楚，在其他 hart 中运行的终端线程为何会认为自己仍然在 hart0 上？很有可能是目前获取 `cpuid()` 的方式是直接查询 `x3(gp)` 寄存器。

现在发现了更为明显的问题：明明被其他 hart 抢走了，但是第 3 步 `sys_read` 返回的时候却依然显示认为自己在 hart0 上。也就是说明其他 hart 的 `gp` 寄存器被改成 0 了？

### rCore SMP support

### RISC-V/K210 多核支持

### 常见的同步互斥原语

* spinlock 依赖中断、原子指令
* sleeplock 依赖 spinlock、线程切换
* condition variable
* semaphore

## 已有思路





