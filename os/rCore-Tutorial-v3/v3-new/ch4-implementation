第四章代码规划。

既然都写了，那肯定就是尽可能多探索一些可能性啊。

需要实现内核动态内存分配。
需要实现用户和内核地址空间隔离。这样做完之后即使内核在低地址空间（比如依然是 0x80020000）上也不会产生冲突，用户程序就直接从 0x0 开始即可。那么内核是在低地址还是高地址呢？和之前兼容还是弄到 0xffffffff80020000 上去吧。
隔离会使得实现和之前有一些不同之处：
1. __alltraps 和 __restore 属于内核的数据段，只有将 satp 切换到内核空间才能访问。虽然修改 stvec 之后，用户能够以 S 特权级跳转到 __alltraps，但是这条指令是没法访问的，因为 satp 还是用户的地址空间。所以 xv6 里面的做法是：将 __alltraps 和 __restore 放在一个 page 里面映射到用户和内核地址空间。这个 page 原先放在内核的数据段里面，在创建地址空间的时候，分配一个物理页框，将这个 page 里面的内容拷贝进去，并映射到虚拟地址空间的最高 page。于是只需将 stvec 设置为这个地址，无论是用户还是内核进入 Trap，都可以访问。

用户在 trampoline 的时候如何知道内核的 satp ，又怎么知道专属的内核栈的位置：可以直接把内核栈的位置放在 sscratch 中，satp 放到 TrapContext 中就好了。需要注意的是，需要将 satp 切换到内核之后才能把寄存器保存到内核栈上的 TrapContext。直接在用户地址空间里面放一个内核的 satp 就好了，并将这个段设置为不允许用户访问。如此的话，在用户 Trap 跳转到 __alltraps 的时候是可以访问的。这样，从特权级来说没有什么安全隐患，但是不知道从处理器层面有没有幽灵/熔断等漏洞。具体还需要看一下 xv6 中的设计。

内核 satp 能在哪里找到？（参考 xv6 设计）

另一个需要斟酌的问题是 TrapContext 到底要放在什么地方。xv6 里面是放在用户空间中的次高页中。但是我还是比较倾向于将其保存在内核栈中。确实 satp 是一个完全的例外...如果预留空间将其保存在trampoline page中是否可行？在设置地址空间的时候，将它的 satp 放进去。然后在 __alltraps 中如何获取？很遗憾的是 sscratch 只能用来保存内核栈的位置。也不一定啊？比如 sscratch 用来保存一块包含了{内核栈地址，satp}数据的地址，它在用户空间中，但是用户无权访问。这样也许可以！想了一下还是不行，因为 sscratch 就只有一个。

这样想来的，好像只能把 TrapContext 丢到用户的次高页里面去了。这样做完之后，用户 Trap 进内核和内核 Trap 进内核的代码有了较大的不同，因此只能拆成两段代码。


用户/内核栈的动态/静态分配？

2. 在内核修改用户地址空间没有那么容易，需要手动查用户页表来获得物理地址。如果要读写大量数据只能逐页。但是这并不构成什么问题。

3. 地址空间分布和以往不同。内核地址空间存放进程的内核栈（以及可能的内核线程的栈），还有 trampoline page，以及线性映射的整块物理内存（包括内核代码、数据段和其他部分），此外，还有恒等映射的设备 MMIO。用户地址空间存放用户代码、数据段，堆空间和最高的 trampoline page 还有次高的用户栈。

一些值得讨论的问题：
1. 之后希望支持内核线程来做一些（多核）同步互斥的内容。目前设计的话，内核线程和用户进程可以共存，分别命名 KernelThread 和 Process。它们的 ControlBlock 都需要包含 MemorySet。它们都能够作为调度单位，因此需要一个特别的 Trait 能够丢到调度队列中。如果这样的话，Trap 就存在下面这些可能：
[1] 用户进程从 U 进入 S Trap（中断/异常）
[2] 用户进程从 S 进入 S Trap (目前仅有的一种可能是在处理 syscall 的时候又打开了 S 中断)
[3] 内核线程从 S 进入 S Trap (目前只能是通过中断，也有可能是异常)，内核线程不会使用 syscall 而是直接调用内核接口
因此 Trap 相关处理能否涵盖这些情况需要特别注意。
其中只有 [1] 需要换栈和换 satp，[2][3] 都不需要换栈和 satp，因为本身就已经在内核态了。那么这两种情况能否共用一套 __alltraps 和 __restore？目前还不清楚。xv6 里面是[1] 发生之后跳转到特别处理 U 的 handler，在里面修改 stvec 到一套新的 __alltraps 和 __restore。所以依然可以考虑从 sstatus.spp 判断是否需要换栈，这也是老传世经典了。

MemorySet 的设计是一个相对独立也比较简单的内容。基本上直接沿用原 v3 的设计即可。只不过，在映射空间的类型上需要更加考虑到后面的拓展，如 COW/mmap/sbrk 等等延迟映射

2. 内核堆分配器和物理页帧分配器将也许是第一次用到同步互斥/锁的概念。有趣的是，BlogOS 的第一版使用原子类型实现了一个无锁分配器。不过，这不是我们现在要做的事情。
在单核且只有应用程序的情况下，看上去应当不存在任何潜在的并发冲突。

代码总体目标：允许内核线程和用户进程同时存在（为了验证 trampoline 机制实现的正确性和拓展性），并可以同时放到调度队列中。实现内核和用户地址空间的隔离。支持动态加载 ELF。支持一个更加动态的调度队列。不必特别强调调度算法的接口分离。兼容后续可能的奇怪操作，如.../.../...。

代码分阶段目标：这一章代码似乎是有史以来最难写的一部分...闯过这关后面还有很多关...?
Step1：实现基本的物理内存探测和物理页帧分配机制。[需要说明的是，它只支持以单个页面为单位进行分配，某种程度上可能降低 cache 性能？但现在不用管。]
Step2：实现动态内存分配机制。希望能够自己实现一个 buddy system allocator。事实上如果开头有一定偏移量的话可能还不太容易实现。
说明1：Step1/2 中的分配器目前可以考虑使用 unsafe impl Sync 来绕过去，因为现在并不存在 Trap 嵌套。只有到最后极端复杂的时候才会考虑 syscall+时钟中断这种嵌套。
Step3：实现页表机制。和第三版一样，MemorySet 表示一个地址空间（用户/内核），里面包含若干个 MemoryArea，每个都是一个特定映射方式的连续虚拟地址空间。目前可能的映射方式有：线性映射 Linear，逐页映射 Framed，文件映射 Mmap，等等。MemorySet 还需要对页表相关的所有分配的物理页帧进行管理，使得用户进程退出之后这些页面能够被及时释放。此外还有一些常规查页表操作。这次我希望能纯粹一点，不要搞一个 HashSet 而是每次手动去查。
说明2：后面还需要兼容虚拟存储，于是这个应该如何修改呢？也就是说，每个进程在内存中常驻的存放数据（而非页表）的物理页面数量是受到限制的。一旦超出了就需要暂时换出到磁盘的一个文件中。[另：已有的对于使用标志位的奇怪操作：sbrk延迟分配/COW/甚至是MMAP/]
Step4：将 MemorySet 插入到 TaskControlBlock 中，修改用户程序链接脚本为 0x0 开头，然后将 ELF 而不是 bin 链接到内核，并实现运行时的动态加载。
Step5：实现 trampoline 机制在 trap 的时候完成地址空间的切换。此外，还需要划分 U/S Trap 到 S 的代码，不仅 TrapContext 保存/恢复的汇编代码不同，处理入口也不同。

太懒了，去看一下基于链表的 buddy system allocator 的实现吧。花了一点时间看了一下 xv6 libc 里面的基于 malloc 的 sbrk 的实现，就是在可用区域上面搞了一个 header 的链表，和物理页帧分配器差不多。每次扫描链表然后用 first-fit 进行分配。每次发现可用空间不足的时候就直接 sbrk，free 的时候也不会通过 sbrk 缩小空间。然后尝试去理解 Rust 的 alloc 为何需要一个对齐需求（其实 C 的 malloc 也是需要的，只是都不超过一个很小的常数，如 double 的 8 字节），但是未果。然后自己写太复杂了，完——全——弃疗。

新一阶段的实现计划：
1. 完成内核重映射
2. 解析每个ELF并新建地址空间
3. 完成 trampoline/trapframe 的相应处理。注意任务切换会切换内核栈，很重要的似乎是 current app，这能够知道当前应用的 satp，但好像不尽然？
难点：内核栈的动态分配
